# talk-to-brief
ğŸ¤ Talk-to-Brief: From 30-Min Audio â†’ 1-Page Insights in Minutes


# ğŸ¤ Talk-to-Brief  
From 30-Min Conference Audio â†’ 1-Page Brief with Questions & Highlights in Minutes  

**Goal:** Walk out of any talk with:  
- A **3-line Approach Script** to talk to the speaker  
- **5 Timestamped Questions** that show you actually listened  
- **8â€“12 Key Highlights** with `[mm:ss]` references  
- **Claims / Assumptions / Trade-offs** to guide real conversation  

All in **â‰¤7 min** after the talk ends.  

---

## ğŸš€ How It Works (80/20 Flow)
1. **Record** 30 min audio â†’ `talk.wav` (any recorder works)  
2. **Transcribe locally** with [faster-whisper](https://github.com/SYSTRAN/faster-whisper) â†’ `transcript.txt + timestamps`  
3. **Analyze with GPT-5** in two passes:
   - **Pass 1:** Run per-chunk prompt â†’ 4 partial outputs  
   - **Pass 2:** Merge prompt â†’ single `brief.md`  
4. **Skim Brief.md** â†’ Walk up to the speaker with context + sharp questions  

---

## ğŸ“‚ Repo Structure
```

talk-to-brief/
â”œâ”€â”€ record.sh             # optional: 30-min recorder script
â”œâ”€â”€ transcribe.py         # audio â†’ transcript + timestamps
â”œâ”€â”€ analyze\_chunk.py      # per-chunk GPT-5 call
â”œâ”€â”€ merge\_brief.py        # final merge GPT-5 call
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ per\_chunk.md      # per-chunk prompt (Tier-1 only)
â”‚   â””â”€â”€ merge.md          # merge prompt
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ transcript.txt
â”‚   â”œâ”€â”€ chunks/
â”‚   â”œâ”€â”€ partials/
â”‚   â””â”€â”€ brief.md
â””â”€â”€ README.md

````

---

## ğŸ›  Setup
```bash
git clone https://github.com/yourname/talk-to-brief.git
cd talk-to-brief
pip install -r requirements.txt
````

* Get a [GPT-5 API key](#) and set it as `OPENAI_API_KEY`.
* Install `faster-whisper` for transcription.

---

## ğŸƒ Usage

```bash
# 1. Record 30 min talk (or use any WAV recorder)
bash record.sh

# 2. Transcribe
python transcribe.py talk.wav

# 3. Analyze chunks in parallel
python analyze_chunk.py

# 4. Merge into final brief
python merge_brief.py

# 5. Open the brief
open outputs/brief.md
```

---

## ğŸ¯ Output Example

```
# Approach Script
"At 12:47 you mentioned cold vector latency. I'd love to hear why PQ over OPQ was the final call â€” we hit similar issues in our own pipelines."

# Five High-Signal Questions
1. [12:47] Why PQ vs OPQ for latency optimization?  
2. [18:05] How did you balance cost vs recall here?  
...

# Timeline Highlights
- [05:20] Introduced hybrid index approach
- [12:47] Latency benchmarks on cold vectors
...

# Key Claims / Assumptions / Trade-offs
Claims:
- Hybrid indexing reduced cold start latency by 40%
Assumptions:
- NVMe storage cost is acceptable at this scale
Trade-offs:
- Higher indexing latency during ingestion
```

---

## ğŸ§  Why This Matters

You get **specific, timestamped context** + **conversation-ready questions** without sifting through 30 min of audio manually.

Perfect for:

* Conferences
* Guest lectures
* Tech meetups

---

## ğŸ“œ License

MIT
