{
  "chunk_index": 0,
  "chunk_text": "[00:03 -> 00:22] Welcome everyone. I'm going to talk about practical tactics to build reliable AI applications.\n[00:23 -> 00:29] And why nobody does it this way yet. A little bit about myself or why you should trust\n[00:29 -> 00:38] me. I've allowed 15 years as a startup co-founder and a CEO. I held executive positions for\n[00:38 -> 00:44] the last five years at several enterprises. But most importantly, I spent last couple of years\n[00:44 -> 00:54] developing a lot of genre projects ranging from POCs to many production level solutions\n[00:54 -> 01:04] and helped some companies to get it done. And I've learned or distilled a way to make these\n[01:04 -> 01:13] applications reliable. And there are quite a lot of tracks, this conference about\n[01:13 -> 01:21] evals and reliability. But to my surprise, nobody was talking about the most important things.\n[01:22 -> 01:28] And we're going to talk about it right now. So, standard software development lifecycle\n[01:28 -> 01:36] is a very standard simple. You design your solution, you develop it, you test it, and then\n[01:36 -> 01:47] eventually you deploy it. And when people start doing POC with AI, it sounds simple. You can\n[01:47 -> 01:56] very easily do some prompt and models are very capable. But then you start facing some unexpected\n[01:56 -> 02:05] challenges. Actually, you can easily do a POC that works 50% of the time. But you're making it\n[02:05 -> 02:13] do the same reliable work. The rest of it 50% is very hard because models are non-deterministic.\n[02:14 -> 02:21] And it starts requiring a data science approach. Continuous experimentation, you need to try\n[02:21 -> 02:25] this prompt, you need to try that model, you need to try this approach, et cetera, et cetera.\n[02:26 -> 02:33] And everything in your solution, everything that represents your solution, which is your code,\n[02:33 -> 02:39] your logic, the prompt, the models that you use, the data that you base your solution on,\n[02:40 -> 02:46] changing anything of that impacts your solution in unexpected ways.\n[02:50 -> 03:00] People often come to this to try solving this with the wrong approach. They start with data science\n[03:00 -> 03:06] metrics. It sounds reasonable, so it requires data science approach of the experimentation,\n[03:07 -> 03:16] and people start measuring groundness, factuality, bias, and other metrics that don't really help\n[03:16 -> 03:24] you to understand is your solution working the right way? Does your latest change\n[03:25 -> 03:32] improve your solution in the right way for your users? For example, I've been talking to an\n[03:32 -> 03:38] ex-colleague that are building a customer support boat at weeks. I asked him, how do you know that\n[03:38 -> 03:44] your solution is working well? He started talking about factuality and other data science metrics.\n[03:46 -> 03:52] That's again, I started to dig deeper, and then we just together figure out that the most\n[03:52 -> 04:01] important metric for them is the rate of moving from AI support boat, like escalation to a human\n[04:01 -> 04:10] support. If your solution hasn't able to answer the user with all this factuality, it could be\n[04:10 -> 04:16] super grounded, but still not provide the right answer that the user expects, and this is what\n[04:16 -> 04:28] you actually need to test. My experience was to start with real-world scenarios. Basically,\n[04:28 -> 04:35] you need to reverse engineer your metrics, and your metrics should be very specific to what\n[04:35 -> 04:43] your end goal should come from a product experience from business outcomes. If your solution is\n[04:43 -> 04:48] customer support boat, you need to figure out what your users want and how you can mimic it,\n[04:48 -> 04:55] and instead of measuring something other age or something generic, you need to measure\n[04:55 -> 05:04] a very specific criteria, because universities don't really work. How do we do it?\n[05:04 -> 05:10] So, for example, customer support boat, which is by way one of the hardest things to do\n[05:10 -> 05:20] evolves properly. Let's have a bank, and a bank has FAQ materials, which contain, including\n[05:20 -> 05:30] how do you reset your password. So what I usually do when I help my companies, but I help them to\n[05:30 -> 05:37] build AI solutions, we start with reverse engineering, like how do we create VALs based on that.\n[05:38 -> 05:44] So in this case, I use LLM, and in most cases I use LLM, to come up with right evaluations.\n[05:45 -> 05:54] So here I can take, say, O3 now, and just reverse engineer what should be the user question\n[05:54 -> 06:00] that we know to answer based on these materials, and what should be the specific criteria that\n[06:01 -> 06:07] these materials provide an answer for. And some of these criteria are quite important. So,\n[06:07 -> 06:13] for example, here it says that as part of the thing, you need to receive a mobile validation,\n[06:13 -> 06:21] so you receive a SMS code, and it says that if you don't have a mobile number, then you can reach\n[06:21 -> 06:27] support, et cetera, et cetera. If some of that information is missing from the answer,\n[06:28 -> 06:34] the answer would not be correct, but you need to be very specific about what exact information\n[06:34 -> 06:40] you need to see in the answer, and that information is very specific to that specific question.\n[06:40 -> 06:47] So you need to build like lots of VALs from the materials, in this case,\n[06:48 -> 06:54] that mimic specific user questions, that you need to be able to answer for.\n[06:55 -> 07:03] How do we do it usually? Again, I work with smart models like O3, and I provided enough context,\n[07:04 -> 07:10] I provided which personas are we trying to represent, because you can make ask the same question\n[07:10 -> 07:18] in completely different ways, depending on who is the persona asking, yet you would expect exactly\n[07:18 -> 07:27] the same answer, so you need to account for it. So this is an example from the open source platform\n[07:27 -> 07:34] that we have that just helps to get it done. So if you look it up, Multineer, I'm not trying to\n[07:34 -> 07:39] sell you anything, I'm not trying to like vendor lock in or whatever, it's completely open source,\n[07:39 -> 07:45] and if needed, I can just recreate it in a couple of days now, with cursor, the point is in the\n[07:45 -> 07:53] approach, not in the platform. So for example here, we see that very same question, how do I\n[07:53 -> 08:01] reset my password? You see what was the input, what was the output, and that specific criteria that\n[08:01 -> 08:08] I measure it? That specific question, how do I know if the answer is correct?",
  "analysis": "## 1) **Approach Script (3 sentences)**\nWelcome to the exploration of practical tactics for building reliable AI applications. With a wealth of experience in startups, enterprises, and AI projects, we'll delve into the challenges and solutions of creating dependable AI systems. Let's unravel the overlooked aspects of AI reliability and the importance of real-world scenarios and specific metrics in this journey.\n\n## 2) **Five High-Signal Questions**\n- [00:38 -> 00:44] How has your executive experience in enterprises influenced your approach to AI reliability?\n- [01:04 -> 01:13] What are the key factors that make an AI application reliable?\n- [01:56 -> 02:05] Can you elaborate on the challenges faced when transitioning from a POC to a reliable AI application?\n- [03:32 -> 03:38] How do data science metrics fall short in assessing the effectiveness of AI solutions?\n- [04:28 -> 04:35] Can you provide an example of how to reverse engineer metrics for a specific AI application?\n\n## 3) **Timeline Highlights (8â€“12 bullets)**\n- [00:03 -> 00:22] Introduction to the talk about building reliable AI applications.\n- [00:23 -> 00:29] Mention of the speaker's unique approach to AI reliability.\n- [00:29 -> 00:38] Speaker's background as a startup co-founder and CEO.\n- [01:04 -> 01:13] Introduction to the concept of AI application reliability.\n- [01:56 -> 02:05] Discussion on the challenges of transitioning from a POC to a reliable AI application.\n- [02:14 -> 02:21] Introduction to the data science approach to AI reliability.\n- [03:25 -> 03:32] Critique of traditional data science metrics in evaluating AI solutions.\n- [04:28 -> 04:35] Introduction to the concept of reverse engineering metrics for AI applications.\n- [05:04 -> 05:10] Example of a challenging AI application: a customer support bot.\n- [06:07 -> 06:13] Discussion on the importance of specific criteria in evaluating AI solutions.\n- [07:27 -> 07:34] Introduction to the open-source platform, Multineer, for AI reliability.\n\n## 4) **Key Claims, Assumptions, Trade-offs**\n- **Claims (assertions)**\n  - AI applications can be made reliable by using real-world scenarios and specific metrics.\n  - Traditional data science metrics often fail to accurately assess the effectiveness of AI solutions.\n  - Reverse engineering metrics based on the end goal can improve AI reliability.\n- **Assumptions (constraints implied)**\n  - AI reliability is a largely overlooked aspect in the current AI landscape.\n  - The speaker's experience and approach to AI reliability is unique and effective.\n  - The open-source platform, Multineer, can aid in improving AI reliability.\n- **Trade-offs (gains vs sacrifices)**\n  - Gains: Improved AI reliability, better understanding of AI solutions, and more effective AI applications.\n  - Sacrifices: Time and effort spent on reverse engineering metrics and creating real-world scenarios.",
  "word_count": 1190,
  "success": true
}