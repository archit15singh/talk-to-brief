{
  "chunk_index": 1,
  "chunk_text": "[08:09 -> 08:14] And now I can just reiterate and generate like 50 different variations of the same question,\n[08:14 -> 08:21] and see if I still get the right answer, if the answer matches all the checklist that I have\n[08:21 -> 08:31] for that specific answer. How the process usually works? So contrary to regular approach,\n[08:31 -> 08:36] you build your evolves, not at the end of the process, but at the very beginning of the process.\n[08:37 -> 08:45] So you just build your first version of the POC, you define the first version of your tests,\n[08:45 -> 08:52] evaluations, you run them, and you see what's going on. You will see that in some cases it will fail,\n[08:52 -> 08:59] in some cases it will succeed. What's important is to look at the details, not just see the average\n[08:59 -> 09:05] numbers, the average numbers won't tell you anything, won't tell you how to improve it.\n[09:05 -> 09:11] If you actually look at the details of each evaluation, you'll see exactly why it's failing. It could be\n[09:11 -> 09:18] failing because your test is not defined correctly. It could be failing because your solution is not\n[09:18 -> 09:26] working as it should be, and like in order to do it, you may need to do a change in, like you may\n[09:26 -> 09:31] change your model, you may change something in your logic, you may change a prompt, or the data\n[09:31 -> 09:39] that you use in order to answer a question in our example. And basically what you do now is\n[09:39 -> 09:45] experimentation, so you start running your experiment, you change something, you need to define\n[09:45 -> 09:54] these tests in a way that will help you to make an educated guess on what you need to change in\n[09:54 -> 09:59] order to do it, in some cases it will work, in some cases it won't. But even if it works,\n[10:00 -> 10:07] let's say you change something in your prompt, and it fixed this test. In my experience, in many cases it\n[10:07 -> 10:15] breaks something that used to work before, like you have constant regressions, and if you don't have\n[10:15 -> 10:21] visualizations, there is no way you'll be able to catch it on time. So this is hugely important,\n[10:21 -> 10:26] and what actually happens is that again, you build your first version, you build your first version\n[10:26 -> 10:32] of evals, you match them, you run vis-a-vals, you improve something, you improve your evals, or maybe\n[10:32 -> 10:39] add more evaluations, and then you like continuously improve it until you reach some point where you are\n[10:39 -> 10:47] satisfied with your evals, for this specific solution, for that specific point of time. And what\n[10:47 -> 10:53] actually happened is that you got your baseline, you got your benchmark, that now you can start\n[10:53 -> 11:01] optimizing, and you have the confidence that the tests should be working, so now you can try\n[11:01 -> 11:09] another model, let's say, well, what, how can I try to see if Foro Mini will work the same way with\n[11:09 -> 11:17] Foro, or not, can I use the graphrag, or can I try a simpler solution, should I have to use\n[11:17 -> 11:27] an agentic approach that maybe better, but requires more time, more inference cost, etc., or should\n[11:27 -> 11:32] they try to simplify the logic, or maybe I can simplify the logic for a specific portion of the\n[11:32 -> 11:40] application, etc., etc., having this benchmark allows you to do all visit limitations with confidence,\n[11:40 -> 11:47] but again, the most important part is like, how do you reach this benchmark, and while the approach\n[11:47 -> 11:54] is pretty much the same, the evaluations that you need to build, and how do you build your evaluations\n[11:54 -> 12:00] are completely different depending on the solution that you need to build, because the models are\n[12:00 -> 12:07] super capable right now, so they allow you to build a huge variety of solutions, but each and every\n[12:07 -> 12:14] solution is quite different in terms of how do you evaluate it. For support bot, you usually,\n[12:14 -> 12:21] typically use LLM as a judge, as I made an example, if your building text to SQL, or text to graph\n[12:21 -> 12:28] database, then to my experience, the best way is to create a mock database that represents the\n[12:30 -> 12:36] database, whatever database or databases that you need your solution to work with, they represent\n[12:36 -> 12:42] the same schema, and you have the mock data, so we know exactly what should expect on specific\n[12:42 -> 12:49] questions. If you need to build some classifier for call center conversations, then your tests are\n[12:49 -> 12:57] like simple much, whenever this is the right rubric or not, and this approach applies to guardrails,\n[12:57 -> 13:06] so getting back to the example of customer support bot, guardrails, you need to cover questions\n[13:06 -> 13:11] that should not be answered, or questions that should be answered in different ways, or questions\n[13:11 -> 13:17] that the answers are not in the material, so all of these you can put into your benchmark,\n[13:17 -> 13:25] just different type of benchmark, but it's pretty much the same approach. So just to reiterate\n[13:25 -> 13:30] the key takeaways, you need to evaluate your apps the way your users actually use them,\n[13:31 -> 13:38] and avoid abstract metrics, because these abstract vectors don't really measure anything important,\n[13:39 -> 13:44] and the approach is for experimentation, so you run these evaluations frequently,\n[13:44 -> 13:52] that allows you to have rapid progress with less regressions, because testing frequently\n[13:52 -> 13:59] help you to catch with surprises, but most importantly, what you get if you define your evaluations\n[13:59 -> 14:07] correctly, you get your solution pretty much as kind of explainable AI, because you know exactly what\n[14:07 -> 14:14] it does, you know exactly how it does it, if you test it the right way. Thank you very much,\n[14:15 -> 14:22] take a look at Multineer, that's a platform that you can use to run these evaluations,\n[14:23 -> 14:29] you can totally use any other platform, the approach is quite simple, it doesn't require any specific\n[14:29 -> 14:36] platform, I've built Multineer just because no other platform helped me to do it this way,\n[14:36 -> 14:43] to help me with the process of evaluation like end-to-end. I'm working on a startup that does\n[14:43 -> 14:48] reliable AI automation right now, and yeah, thank you very much.",
  "analysis": "## 1) **Approach Script (3 sentences)**\nAt 08:09, the speaker discusses the importance of generating multiple variations of the same question to test the robustness of a solution. The speaker emphasizes the need to look at the details of each evaluation, not just the average numbers, to understand why a solution may be failing. The speaker concludes by advocating for a process of continuous improvement and experimentation, with the goal of reaching a satisfactory benchmark.\n\n## 2) **Five High-Signal Questions**\n- At 08:14, how does the speaker ensure the answer matches all the checklist items for a specific question?\n- At 08:59, why does the speaker believe average numbers won't provide useful insights for improvement?\n- At 10:07, what strategies does the speaker suggest for managing regressions when a change fixes one test but breaks another?\n- At 11:27, how does the speaker propose simplifying the logic for a specific portion of the application?\n- At 13:30, how does the speaker suggest avoiding abstract metrics in favor of evaluating apps the way users actually use them?\n\n## 3) **Timeline Highlights (8â€“12 bullets)**\n- [08:09] Speaker discusses generating multiple variations of the same question for testing.\n- [08:31] Emphasizes building evolves at the beginning of the process.\n- [08:45] Discusses running evaluations and observing results.\n- [09:05] Highlights the importance of looking at the details of each evaluation.\n- [09:18] Suggests potential reasons for a solution failing.\n- [10:00] Discusses the process of experimentation and making changes.\n- [10:21] Emphasizes the importance of visualizations to catch regressions.\n- [11:09] Talks about trying different models and approaches.\n- [12:14] Discusses different evaluation methods for different solutions.\n- [13:25] Reiterates key takeaways, including the importance of user-centric evaluation and frequent testing.\n- [14:15] Introduces Multineer, a platform for running evaluations.\n\n## 4) **Key Claims, Assumptions, Trade-offs**\n- Claims (assertions)\n  - Evaluations should be built at the beginning of the process.\n  - Looking at the details of each evaluation is crucial for understanding failures.\n  - Frequent testing allows for rapid progress with fewer regressions.\n- Assumptions (constraints implied)\n  - The process of building a solution involves continuous improvement and experimentation.\n  - Different solutions require different evaluation methods.\n  - Visualizations are essential for catching regressions.\n- Trade-offs (gains vs sacrifices)\n  - Spending time on generating multiple variations of the same question can improve the robustness of a solution.\n  - Focusing on the details of each evaluation may require more time but can provide valuable insights.\n  - Frequent testing can catch regressions early but may require more resources.",
  "word_count": 1154,
  "success": true
}