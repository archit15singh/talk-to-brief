{
  "chunk_index": 1,
  "chunk_text": "[09:15 -> 09:36] It could be failing because your solution is not working as it should be, and in order to do it, you may need to do a change in your logic, you may change a prompt, or the data that you use in order to answer a question in our example.\n[09:38 -> 09:57] Basically, what you do now is experimentation, so you start running your experiment, you change something, you need to define these tests in a way that will help you to make an educated guess on what you need to change in order to do it, in some cases it will work, in some cases it won't.\n[09:57 -> 10:18] But even if it works, let's say you change something in your prompt, and it fixed this test. In my experience, in many cases it breaks something that used to work before, you have constant regressions, and if you don't have these evaluations, there's no way you'll be able to catch it on time.\n[10:18 -> 10:45] So this is hugely important, and what actually happens is that, again, you build your first version, you build your first version of the evals, you match them, you run these evals, you improve something, you improve your evals, or maybe add more evaluations, and then you continuously improve it until you reach some point where you are satisfied with your evals, for this specific solution, for that specific point of time.\n[10:46 -> 11:10] And what actually happened is that you got your baseline, you got your benchmark, that now you can start optimizing, and you have the confidence that the tests should be working, so now you can try another model, let's say, well, what, how can I try to see if Foro Mini will work the same way with Foro, or not?\n[11:11 -> 11:33] Can I use the graph rag, or can I try a simpler solution? Should I have to use the agentic approach that maybe better, but requires more time, more inference costs, etc., or should they try to simplify the logic, or maybe I can simplify the logic for a specific portion of the application, etc., etc.\n[11:33 -> 12:01] Having this benchmark allows you to do all visit to limitations with confidence, but again, the most important part is how do you reach this benchmark, and while the approach is pretty much the same, the evaluations that you need to build, and how do you build your evaluations are completely different depending on the solution that you need to build, because the models are super capable right now,\n[12:01 -> 12:11] so they allow you to build a huge variety of solutions, but each and every solution is quite different in terms of how do you evaluate it.\n[12:12 -> 12:28] For support bot, you usually typically use lm as a judge, as an example. If your building takes to a scale, or takes to graph database, then to my experience the best way is to create a mock database that represents\n[12:30 -> 12:42] whatever database or databases that you need your solution to work with, represent the same schema, and you have the mock data, so you know exactly what to expect on specific questions.\n[12:43 -> 12:53] If you need to build some classifier for call center conversations, then your tests are simple much, whenever this is the right rubric or not.\n[12:54 -> 13:21] And this approach applies to guardrails, so getting back to the example of customer support bot, guardrails you need to cover questions that should not be answered, or questions that should be answered in different ways, or questions that the answers are not in the material, so all of these you can put into your benchmark, just different type of benchmark, but it's pretty much the same approach.\n[13:23 -> 13:38] So just to reiterate the key takeaways, you need to evaluate your apps the way your users actually use them, and avoid abstract metrics, because these abstract vectors don't really measure anything important.\n[13:39 -> 14:07] And the approach is for experimentation, so you run these evaluations frequently, that allows you to have rapid progress with less regressions, because testing frequently helps you to catch with surprises, but most importantly what you get, if you define your evaluations correctly, you get your solution pretty much as kind of explainable AI, because you know exactly what it does,\n[14:07 -> 14:10] you know exactly how it does it, if you test it the right way.\n[14:13 -> 14:25] Thank you very much, take a look at Multinear, that's a platform that you can use to run these evaluations, you can totally use any other platform.\n[14:25 -> 14:39] The approach is quite simple, it doesn't require any specific platform, I built Multinear just because no other platform helped me to do it this way, to help me with the process of evaluation like end to end.\n[14:41 -> 14:48] I'm working on a startup that does reliable AI automation right now, and yeah, thank you very much.",
  "analysis": "## 1) **Approach Script (3 sentences)**\nI was intrigued by your point at [11:33] about reaching a benchmark and how the evaluations needed to build it vary depending on the solution. It seems like a delicate balance between experimentation and evaluation. Could you elaborate more on how you navigate this process?\n\n## 2) **Five High-Signal Questions**\n- At [09:15], you mentioned changing the logic or data used to answer a question. Can you provide an example of a situation where this was necessary?\n- You talked about running experiments at [09:38]. How do you decide what changes to make in these experiments?\n- At [10:18], you discussed continuously improving until reaching a point of satisfaction. How do you define this point?\n- You mentioned the use of different models at [12:01]. Can you share how you choose the right model for a particular solution?\n- At [14:07], you referred to defining evaluations correctly. What are some common mistakes to avoid in this process?\n\n## 3) **Timeline Highlights (8â€“12 bullets)**\n- [09:15] Discusses potential reasons for failure and changes needed in logic, prompts, or data.\n- [09:38] Introduces the concept of experimentation and educated guessing.\n- [09:57] Talks about the possibility of changes causing regressions.\n- [10:18] Emphasizes the importance of evaluations and continuous improvement.\n- [11:10] Discusses the establishment of a baseline or benchmark for optimization.\n- [11:33] Highlights the importance of reaching a benchmark and the variability in evaluations.\n- [12:12] Gives examples of different evaluations for different solutions.\n- [13:23] Reiterates the importance of evaluating apps as users use them and avoiding abstract metrics.\n- [14:07] Discusses the benefits of defining evaluations correctly.\n- [14:25] Introduces Multilinear, a platform for running evaluations.\n\n## 4) **Key Claims, Assumptions, Trade-offs**\n- Claims:\n  - Evaluations are crucial for continuous improvement and catching regressions.\n  - Establishing a benchmark allows for confident optimization.\n  - Evaluations should be defined correctly to understand exactly what a solution does.\n- Assumptions:\n  - The process of reaching a benchmark involves the same approach but different evaluations depending on the solution.\n  - Testing frequently helps catch surprises and allows for rapid progress with less regressions.\n  - Abstract metrics don't measure anything important.\n- Trade-offs:\n  - The process of experimentation and evaluation may lead to regressions.\n  - Choosing a more complex approach may yield better results but requires more time and resources.\n  - Simplifying the logic may be beneficial for a specific portion of an application but may not work for the entire application.",
  "word_count": 855,
  "success": true
}