{
  "chunk_index": 1,
  "chunk_text": "[08:14 -> 08:21] and see if I still get the right answer, if the answer matches all the check list that\n[08:21 -> 08:30] I have for that specific answer. How the process usually works? So contrary to like\n[08:30 -> 08:35] regular approach, you build your evals not at the end of the process, but at the very\n[08:35 -> 08:43] beginning of the process. So just build your first version of the POC, you define the\n[08:43 -> 08:49] first version of your test evaluations, you run them and you see what's going on, you will\n[08:49 -> 08:56] see that in some cases it will fail, in some cases it will succeed. What's important is\n[08:56 -> 09:01] to look at the details, not just see the average numbers, the average numbers won't\n[09:01 -> 09:07] tell you anything, won't tell you how to improve it. If you actually look at the details\n[09:07 -> 09:11] of each evaluation, you'll see exactly why it's failing, it could be failing\n[09:12 -> 09:18] because your test is not defined correctly, it could be failing because your solution is\n[09:18 -> 09:25] not working as it should be, and like in order to do it, you may need to do a change in,\n[09:25 -> 09:29] like you may change a model, you may change something in your logic, you may change\n[09:29 -> 09:37] a prompt or the data that you use in order to answer a question in our example. And\n[09:38 -> 09:43] basically what you do now is experimentation, so you start running your experiment, you\n[09:43 -> 09:50] change something, you need to define these tests in a way that will help you to make an\n[09:50 -> 09:56] educated guess on what you need to change in order to do it, in some cases it will\n[09:56 -> 10:02] work, in some cases it won't, but even if it works, let's say you change something\n[10:02 -> 10:08] in your prompt and it fixed this test. In my experience in many cases it breaks something\n[10:08 -> 10:15] that used to work before, like you have constant regressions and if you don't have\n[10:15 -> 10:20] these evaluations there's no way you'll be able to catch it on time. So this is hugely\n[10:20 -> 10:25] important and what actually happens is that, again, you build your first version, you\n[10:25 -> 10:30] build your first version of the VALS, you match them, you run these VALS, you improve\n[10:30 -> 10:35] something, you improve your VALS, or maybe add more evaluations and then you continuously\n[10:35 -> 10:42] improve it until you reach some point where you are satisfied with your VALS for this\n[10:42 -> 10:48] specific solution, for that specific point of time. And what actually happened is that\n[10:48 -> 10:56] you got your baseline, you got your benchmark that now you can start optimizing and you\n[10:56 -> 11:02] have the confidence that the tests should be working, so now you can try another model.\n[11:02 -> 11:10] Let's say, well, how can I try to see if 4.0 Mini will work the same way with 4.0 or\n[11:10 -> 11:17] not? Can I use the GraphRug or can I try a simpler solution? Should I have to use\n[11:17 -> 11:26] the agentic approach that may be better but requires more time, more inference cost, etc.\n[11:26 -> 11:31] Or should I try to simplify the logic or maybe I can simplify the logic for a specific portion\n[11:31 -> 11:38] of the application, etc., etc. Having this benchmark allows you to do all these\n[11:38 -> 11:43] experimentations with confidence, but again, the most important part is how do you\n[11:43 -> 11:52] reach this benchmark? And while the approach is pretty much the same, the evaluations that\n[11:52 -> 11:55] you need to build and how do you build your evaluations are completely different depending\n[11:55 -> 12:02] on the solution that you need to build because the models are super capable right now, so\n[12:02 -> 12:07] they allow you to build a huge variety of solutions, but each and every solution\n[12:07 -> 12:14] is quite different in terms of how do you evaluate it. For support bot, you usually typically\n[12:14 -> 12:21] use LLM as a judge as an example. If you're building text to SQL or text to graph database,\n[12:22 -> 12:29] then to my experience, the best way is to create a mock database that represents whatever\n[12:31 -> 12:36] database or databases that you need your solution to work with. They represent the\n[12:36 -> 12:42] same schema and you have the mock data, so you know exactly what to expect on specific\n[12:42 -> 12:49] questions. If you need to build some classifier for call center conversations, then your tests\n[12:49 -> 12:56] are like simple match whenever this is the right rubric or not. And this approach applies\n[12:56 -> 13:04] to guardrails, so getting back to the example of customer support bot, guardrails, you\n[13:04 -> 13:10] need to cover questions that should not be answered or questions that should be answered in different\n[13:10 -> 13:16] ways or questions that the answers are not in the material, so all of this you can put\n[13:16 -> 13:20] into your benchmark, just different type of benchmark, but it's pretty much the same\n[13:20 -> 13:28] approach. So just to reiterate the key takeaways, you need to evaluate your apps the way\n[13:28 -> 13:37] your users actually use them and avoid abstract metrics because these abstract metrics don't\n[13:37 -> 13:42] really measure anything important. And the approach is for experimentation, so you run\n[13:42 -> 13:49] these evaluations frequently, that allows you to have rapid progress with less regressions\n[13:50 -> 13:56] because testing frequently help you to catch these surprises, but most importantly, what\n[13:56 -> 14:04] you get if you define your evaluations correctly, you get your solution pretty much as kind of\n[14:04 -> 14:09] explainable AI because you know exactly what it does, you know exactly how it does it\n[14:09 -> 14:18] if you test it the right way. Thank you very much. Take a look at Multineer, that's\n[14:19 -> 14:25] a platform that you can use to run these evaluations. You can totally use any other\n[14:25 -> 14:31] platform. The approach is quite simple, it doesn't require any specific platform. I've\n[14:31 -> 14:37] built Multineer just because no other platform helped me to do it this way, to help me with\n[14:37 -> 14:43] the process of evaluation like end to end. I'm working on a startup that does reliable\n[14:43 -> 14:48] AI automation right now, and yeah, thank you very much.",
  "analysis": "## 1) **Approach Script (3 sentences)**\n\n\"Let's revisit the moment at [08:35] where you mentioned the importance of building evaluations at the beginning of the process, not at the end. This seems to be a departure from the regular approach. Could you explain how this early evaluation contributes to the overall success of the project?\"\n\n## 2) **Five High-Signal Questions**\n- At [08:56], you mentioned the importance of looking at the details of each evaluation. Could you provide an example of a detail that greatly impacted the outcome of a project?\n- At [09:18], you talked about the possibility of the solution not working as it should. How do you determine whether the issue lies in the test definition or the solution itself?\n- At [10:56], you mentioned having a baseline or benchmark for optimization. How do you decide what makes a good benchmark for a particular project?\n- At [11:26], you brought up the possibility of simplifying the logic for a specific portion of the application. Can you share an instance where such simplification resulted in significant improvements?\n- At [13:28], you emphasized the need to evaluate apps the way users actually use them. How do you ensure that your evaluations accurately reflect user behavior?\n\n## 3) **Timeline Highlights (8â€“12 bullets)**\n- [08:14] Discusses checking if the answer matches the checklist.\n- [08:35] Highlights the importance of building evaluations at the beginning of the process.\n- [09:01] Stresses the importance of looking at the details of each evaluation.\n- [09:18] Talks about potential reasons for test failures.\n- [10:20] Discusses the importance of evaluations in catching regressions.\n- [10:56] Introduces the concept of having a baseline or benchmark for optimization.\n- [11:26] Discusses the possibility of simplifying the logic for a specific portion of the application.\n- [13:28] Emphasizes the need to evaluate apps the way users actually use them.\n- [14:04] Talks about the benefits of defining evaluations correctly.\n- [14:19] Recommends Multineer as a platform for running evaluations.\n\n## 4) **Key Claims, Assumptions, Trade-offs**\n- **Claims (assertions)**\n  - Evaluations should be built at the beginning of the process, not at the end.\n  - Looking at the details of each evaluation is crucial for improvement.\n  - Having a baseline or benchmark is essential for optimization.\n- **Assumptions (constraints implied)**\n  - The process of building evaluations is iterative and involves continuous improvement.\n  - The approach to evaluation should be tailored to the specific solution being built.\n  - The evaluations should reflect how the users actually use the applications.\n- **Trade-offs (gains vs sacrifices)**\n  - Investing time in building evaluations early on can lead to rapid progress with fewer regressions.\n  - Simplifying the logic for a specific portion of the application may improve performance but could also limit functionality.\n  - Using a specific platform for running evaluations may offer convenience but could limit flexibility.",
  "word_count": 1149,
  "success": true
}