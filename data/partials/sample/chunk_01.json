{
  "chunk_index": 1,
  "chunk_text": "[08:14 -> 08:21] and see if I still get the right answer, if the answer matches all the check list that\n[08:21 -> 08:30] I have for that specific answer. How the process usually works? So contrary to like\n[08:30 -> 08:35] regular approach, you build your evals not at the end of the process, but at the very\n[08:35 -> 08:43] beginning of the process. So just build your first version of the POC, you define the\n[08:43 -> 08:49] first version of your test evaluations, you run them and you see what's going on, you will\n[08:49 -> 08:56] see that in some cases it will fail, in some cases it will succeed. What's important is\n[08:56 -> 09:01] to look at the details, not just see the average numbers, the average numbers won't\n[09:01 -> 09:07] tell you anything, won't tell you how to improve it. If you actually look at the details\n[09:07 -> 09:11] of each evaluation, you'll see exactly why it's failing, it could be failing\n[09:12 -> 09:18] because your test is not defined correctly, it could be failing because your solution is\n[09:18 -> 09:25] not working as it should be, and like in order to do it, you may need to do a change in,\n[09:25 -> 09:29] like you may change a model, you may change something in your logic, you may change\n[09:29 -> 09:37] a prompt or the data that you use in order to answer a question in our example. And\n[09:38 -> 09:43] basically what you do now is experimentation, so you start running your experiment, you\n[09:43 -> 09:50] change something, you need to define these tests in a way that will help you to make an\n[09:50 -> 09:56] educated guess on what you need to change in order to do it, in some cases it will\n[09:56 -> 10:02] work, in some cases it won't, but even if it works, let's say you change something\n[10:02 -> 10:08] in your prompt and it fixed this test. In my experience in many cases it breaks something\n[10:08 -> 10:15] that used to work before, like you have constant regressions and if you don't have\n[10:15 -> 10:20] these evaluations there's no way you'll be able to catch it on time. So this is hugely\n[10:20 -> 10:25] important and what actually happens is that, again, you build your first version, you\n[10:25 -> 10:30] build your first version of the VALS, you match them, you run these VALS, you improve\n[10:30 -> 10:35] something, you improve your VALS, or maybe add more evaluations and then you continuously\n[10:35 -> 10:42] improve it until you reach some point where you are satisfied with your VALS for this\n[10:42 -> 10:48] specific solution, for that specific point of time. And what actually happened is that\n[10:48 -> 10:56] you got your baseline, you got your benchmark that now you can start optimizing and you\n[10:56 -> 11:02] have the confidence that the tests should be working, so now you can try another model.\n[11:02 -> 11:10] Let's say, well, how can I try to see if 4.0 Mini will work the same way with 4.0 or\n[11:10 -> 11:17] not? Can I use the GraphRug or can I try a simpler solution? Should I have to use\n[11:17 -> 11:26] the agentic approach that may be better but requires more time, more inference cost, etc.\n[11:26 -> 11:31] Or should I try to simplify the logic or maybe I can simplify the logic for a specific portion\n[11:31 -> 11:38] of the application, etc., etc. Having this benchmark allows you to do all these\n[11:38 -> 11:43] experimentations with confidence, but again, the most important part is how do you\n[11:43 -> 11:52] reach this benchmark? And while the approach is pretty much the same, the evaluations that\n[11:52 -> 11:55] you need to build and how do you build your evaluations are completely different depending\n[11:55 -> 12:02] on the solution that you need to build because the models are super capable right now, so\n[12:02 -> 12:07] they allow you to build a huge variety of solutions, but each and every solution\n[12:07 -> 12:14] is quite different in terms of how do you evaluate it. For support bot, you usually typically\n[12:14 -> 12:21] use LLM as a judge as an example. If you're building text to SQL or text to graph database,\n[12:22 -> 12:29] then to my experience, the best way is to create a mock database that represents whatever\n[12:31 -> 12:36] database or databases that you need your solution to work with. They represent the\n[12:36 -> 12:42] same schema and you have the mock data, so you know exactly what to expect on specific\n[12:42 -> 12:49] questions. If you need to build some classifier for call center conversations, then your tests\n[12:49 -> 12:56] are like simple match whenever this is the right rubric or not. And this approach applies\n[12:56 -> 13:04] to guardrails, so getting back to the example of customer support bot, guardrails, you\n[13:04 -> 13:10] need to cover questions that should not be answered or questions that should be answered in different\n[13:10 -> 13:16] ways or questions that the answers are not in the material, so all of this you can put\n[13:16 -> 13:20] into your benchmark, just different type of benchmark, but it's pretty much the same\n[13:20 -> 13:28] approach. So just to reiterate the key takeaways, you need to evaluate your apps the way\n[13:28 -> 13:37] your users actually use them and avoid abstract metrics because these abstract metrics don't\n[13:37 -> 13:42] really measure anything important. And the approach is for experimentation, so you run\n[13:42 -> 13:49] these evaluations frequently, that allows you to have rapid progress with less regressions\n[13:50 -> 13:56] because testing frequently help you to catch these surprises, but most importantly, what\n[13:56 -> 14:04] you get if you define your evaluations correctly, you get your solution pretty much as kind of\n[14:04 -> 14:09] explainable AI because you know exactly what it does, you know exactly how it does it\n[14:09 -> 14:18] if you test it the right way. Thank you very much. Take a look at Multineer, that's\n[14:19 -> 14:25] a platform that you can use to run these evaluations. You can totally use any other\n[14:25 -> 14:31] platform. The approach is quite simple, it doesn't require any specific platform. I've\n[14:31 -> 14:37] built Multineer just because no other platform helped me to do it this way, to help me with\n[14:37 -> 14:43] the process of evaluation like end to end. I'm working on a startup that does reliable\n[14:43 -> 14:48] AI automation right now, and yeah, thank you very much.",
  "analysis": "## 1) **Approach Script (3 sentences)**\nAt 08:14, the speaker discusses the importance of building evaluations at the beginning of the process, not the end. They emphasize the need for detailed examination of each evaluation, as it reveals why certain aspects may be failing. The speaker concludes by advocating for frequent evaluations and experimentation, which can lead to rapid progress with fewer regressions.\n\n## 2) **Five High-Signal Questions**\n- At 08:35, you mentioned building evaluations at the beginning of the process. Could you elaborate on the advantages of this approach?\n- How does analyzing the details of each evaluation, as mentioned at 08:56, help improve the overall process?\n- At 10:48, you talked about reaching a benchmark. How does this benchmark guide future experimentation and optimization?\n- Could you explain more about the role of different models in building evaluations, as mentioned at 11:55?\n- At 14:09, you mentioned that the right testing can lead to \"explainable AI\". How does this contribute to the reliability and effectiveness of AI automation?\n\n## 3) **Timeline Highlights (8â€“12 bullets)**\n- [08:14] Speaker discusses the importance of matching answers to a checklist.\n- [08:35] Emphasizes building evaluations at the beginning of the process.\n- [08:56] Stresses the importance of looking at the details of each evaluation.\n- [10:02] Discusses the potential for changes to cause regressions.\n- [10:48] Talks about reaching a benchmark for optimization.\n- [11:55] Discusses the role of different models in building evaluations.\n- [13:20] Reiterates the importance of evaluating apps the way users use them.\n- [13:42] Advocates for frequent evaluations and experimentation.\n- [14:09] Discusses the concept of \"explainable AI\".\n- [14:19] Mentions Multineer, a platform for running evaluations.\n\n## 4) **Key Claims, Assumptions, Trade-offs**\n- **Claims**:\n  - Evaluations should be built at the beginning of the process.\n  - Detailed examination of each evaluation is crucial.\n  - Frequent evaluations and experimentation lead to rapid progress with fewer regressions.\n- **Assumptions**:\n  - The process of evaluation is iterative and requires constant refinement.\n  - Different models may be needed for different solutions.\n  - The right testing can lead to \"explainable AI\".\n- **Trade-offs**:\n  - Building evaluations early might require more upfront work but can lead to more accurate results.\n  - Detailed examination of each evaluation may be time-consuming but provides valuable insights.\n  - Frequent evaluations might slow down the process but can prevent regressions and improve the overall solution.",
  "word_count": 1149,
  "success": true
}