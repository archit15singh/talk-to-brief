{
  "chunk_index": 1,
  "chunk_text": "[08:36 -> 08:54] So you just build your first version of the POC, you define a first version of your tests, evaluations, you run them, and you see what's going on, you will see that in some cases it will fail, in some cases it will succeed.\n[08:54 -> 09:05] What's important is to look at the details, not just see the average numbers, the average numbers won't tell you anything, won't tell you how to improve it.\n[09:05 -> 09:15] If you actually look at the details of each evaluation, you'll see exactly why it's failing, it could be failing because your test is not defined correctly.\n[09:15 -> 09:36] It could be failing because your solution is not working as it should be, and in order to do it, you may need to do a change in your logic, you may change a prompt, or the data that you use in order to answer a question in our example.\n[09:38 -> 09:57] Basically, what you do now is experimentation, so you start running your experiment, you change something, you need to define these tests in a way that will help you to make an educated guess on what you need to change in order to do it, in some cases it will work, in some cases it won't.\n[09:57 -> 10:18] But even if it works, let's say you change something in your prompt, and it fixed this test. In my experience, in many cases it breaks something that used to work before, you have constant regressions, and if you don't have these evaluations, there's no way you'll be able to catch it on time.\n[10:18 -> 10:45] So this is hugely important, and what actually happens is that, again, you build your first version, you build your first version of the evals, you match them, you run these evals, you improve something, you improve your evals, or maybe add more evaluations, and then you continuously improve it until you reach some point where you are satisfied with your evals, for this specific solution, for that specific point of time.\n[10:46 -> 11:10] And what actually happened is that you got your baseline, you got your benchmark, that now you can start optimizing, and you have the confidence that the tests should be working, so now you can try another model, let's say, well, what, how can I try to see if Foro Mini will work the same way with Foro, or not?\n[11:11 -> 11:33] Can I use the graph rag, or can I try a simpler solution? Should I have to use the agentic approach that maybe better, but requires more time, more inference costs, etc., or should they try to simplify the logic, or maybe I can simplify the logic for a specific portion of the application, etc., etc.\n[11:33 -> 12:01] Having this benchmark allows you to do all visit to limitations with confidence, but again, the most important part is how do you reach this benchmark, and while the approach is pretty much the same, the evaluations that you need to build, and how do you build your evaluations are completely different depending on the solution that you need to build, because the models are super capable right now,\n[12:01 -> 12:11] so they allow you to build a huge variety of solutions, but each and every solution is quite different in terms of how do you evaluate it.\n[12:12 -> 12:28] For support bot, you usually typically use lm as a judge, as an example. If your building takes to a scale, or takes to graph database, then to my experience the best way is to create a mock database that represents\n[12:30 -> 12:42] whatever database or databases that you need your solution to work with, represent the same schema, and you have the mock data, so you know exactly what to expect on specific questions.\n[12:43 -> 12:53] If you need to build some classifier for call center conversations, then your tests are simple much, whenever this is the right rubric or not.\n[12:54 -> 13:21] And this approach applies to guardrails, so getting back to the example of customer support bot, guardrails you need to cover questions that should not be answered, or questions that should be answered in different ways, or questions that the answers are not in the material, so all of these you can put into your benchmark, just different type of benchmark, but it's pretty much the same approach.\n[13:23 -> 13:38] So just to reiterate the key takeaways, you need to evaluate your apps the way your users actually use them, and avoid abstract metrics, because these abstract vectors don't really measure anything important.\n[13:39 -> 14:07] And the approach is for experimentation, so you run these evaluations frequently, that allows you to have rapid progress with less regressions, because testing frequently helps you to catch with surprises, but most importantly what you get, if you define your evaluations correctly, you get your solution pretty much as kind of explainable AI, because you know exactly what it does,\n[14:07 -> 14:10] you know exactly how it does it, if you test it the right way.\n[14:13 -> 14:25] Thank you very much, take a look at Multinear, that's a platform that you can use to run these evaluations, you can totally use any other platform.\n[14:25 -> 14:39] The approach is quite simple, it doesn't require any specific platform, I built Multinear just because no other platform helped me to do it this way, to help me with the process of evaluation like end to end.\n[14:41 -> 14:48] I'm working on a startup that does reliable AI automation right now, and yeah, thank you very much.",
  "analysis": "## 1) **Approach Script**\n\"Good morning, everyone. At 09:15, our speaker discussed the importance of understanding the reasons behind the failure of a test, highlighting that it could be due to a poorly defined test or a malfunctioning solution. Isn't it fascinating how these failures can lead to a deeper understanding and subsequent improvement of our systems?\"\n\n## 2) **Five High-Signal Questions**\n1. At 08:54, the speaker mentions the importance of looking at details rather than average numbers. What specific details should we focus on?\n2. How does the speaker suggest modifying the logic or data used in a test at 09:15 when a solution isn't working as expected?\n3. At 10:18, the speaker talks about continuously improving evaluations. What strategies can be employed to ensure this continuous improvement?\n4. The speaker mentions different models at 11:33. How do these models influence the building and evaluation of solutions?\n5. At 13:39, the speaker discusses the importance of frequent evaluations. How can we ensure that these evaluations are done frequently and effectively?\n\n## 3) **Timeline Highlights**\n- [08:36] Speaker discusses building and running the first version of the POC.\n- [08:54] Importance of looking at details, not just average numbers, is emphasized.\n- [09:15] Speaker explains reasons behind test failures.\n- [09:57] The process of experimentation and adjustment is described.\n- [10:18] Speaker talks about continuously improving evaluations.\n- [10:46] Speaker discusses the establishment of a baseline or benchmark.\n- [11:33] Different models and their influence on solution building are mentioned.\n- [12:01] Speaker talks about the variety of solutions that can be built.\n- [12:43] Different types of tests for different solutions are described.\n- [13:23] Speaker reiterates the importance of evaluating apps as users use them.\n- [14:07] Speaker discusses the concept of explainable AI.\n- [14:25] Speaker introduces Multilinear, a platform for running evaluations.\n\n## 4) **Key Claims, Assumptions, Trade-offs**\n- **Claims:**\n  - Evaluations should focus on details, not just average numbers.\n  - Test failures can be due to poorly defined tests or malfunctioning solutions.\n  - Continuous improvement of evaluations is crucial for success.\n- **Assumptions:**\n  - Different models will require different evaluations.\n  - Frequent evaluations lead to rapid progress with fewer regressions.\n  - The process of evaluation is universal and can be applied to a variety of solutions.\n- **Trade-offs:**\n  - Spending time on detailed evaluations can lead to better understanding and improvement of systems.\n  - Experimentation may lead to failures, but these failures can provide valuable insights.\n  - Using a specific platform for evaluations may limit flexibility, but it can streamline the evaluation process.",
  "word_count": 963,
  "success": true
}