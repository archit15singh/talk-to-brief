{
  "chunk_index": 0,
  "chunk_text": "[00:03 -> 00:22] Welcome everyone. I'm going to talk about practical tactics to build reliable AI applications\n[00:23 -> 00:30] and why nobody does it this way yet. A little bit about myself or why you should trust me.\n[00:31 -> 00:42] I'm about 15 years as a startup co-founder and CTO. I held executive positions for the\n[00:42 -> 00:54] last couple of years developing a lot of projects ranging from POCs to many production-level solutions\n[00:54 -> 01:04] and helped some companies to get it done. I've learned or distilled a way to make these\n[01:04 -> 01:15] applications reliable. There are quite a lot of tracks this conference about evals and reliability\n[01:15 -> 01:23] but to my surprise nobody was talking about the most important things and we're going to talk\n[01:23 -> 01:34] about it right now. Standard software development lifecycle is very standard, simple. You design\n[01:34 -> 01:40] your solution, you develop it, you test it and then eventually you deploy it. When people start\n[01:40 -> 01:51] doing POC with AI, it sounds simple. You can very easily do some prompt and models are very\n[01:51 -> 02:01] capable but then you start facing some unexpected challenges. Actually, you can easily do a POC\n[02:01 -> 02:11] that works 50% of the time but making it do the same reliable work the rest of the 50% is very hard\n[02:11 -> 02:20] because models are non-deterministic and it starts requiring a data science approach, continuous\n[02:20 -> 02:24] experimentation. You need to try this prompt, you need to try that model, you need to try this\n[02:24 -> 02:32] approach, etc. Everything in your solution, everything that represents your solution which is your\n[02:32 -> 02:38] code, your logic, the prompts that you use, the models that you use, the data that you base your\n[02:38 -> 02:51] solution on, changing anything of that impacts your solution in unexpected ways. People very\n[02:51 -> 03:00] often come to this, to try solving this with the wrong approach. They start with data science\n[03:00 -> 03:06] metrics. It sounds reasonable, right? So it requires data science approach of the experimentation\n[03:07 -> 03:16] and people start measuring groundness, factuality, bias and other metrics that don't really help\n[03:16 -> 03:24] you to understand is your solution working the right way? Does it, does your latest change\n[03:26 -> 03:31] improved your solution in the right way for your users? For example, I've been talking\n[03:31 -> 03:37] to an ex-colleague that are building a customer support bot at Weeks. I asked him, how do\n[03:37 -> 03:42] you know that your solution is working well? He started talking about factuality and\n[03:42 -> 03:50] other data science metrics. That's again, I started to dig deeper and then we just together\n[03:51 -> 03:59] figure out that the most important metric for them is the rate of moving from AI support\n[03:59 -> 04:07] bot like escalation to a human support. If your solution hasn't able to answer the\n[04:07 -> 04:12] user with all this factuality, like it could be super grounded but still not provide\n[04:12 -> 04:22] the right answer that user expects and this is what you actually need to test. My experience\n[04:22 -> 04:30] was to start with real-world scenarios. Basically, you need to reverse engineer your metrics\n[04:30 -> 04:37] and your metrics should be very, very specific to what your end goal, so they should come\n[04:37 -> 04:44] from a product experience, from business outcomes. If your solution is customer support bot, you\n[04:44 -> 04:51] need to figure out what your users want and how you can mimic it. Instead of measuring\n[04:51 -> 04:57] something average or something generic, you need to measure a very specific criteria\n[04:58 -> 05:06] because universal valves don't really work. How do we do it? For example, customer\n[05:06 -> 05:13] support bot, which is by way one of the hardest things to do the valves properly. Let's say\n[05:13 -> 05:21] I have a bank and a bank has FAQ materials which contain, including how do you reset your\n[05:21 -> 05:31] password? What I usually do when I help my companies that I help them to build AI\n[05:31 -> 05:37] solutions, we start with reverse engineering, like how do we create the valves based on that?\n[05:38 -> 05:44] In this case, I use LLM, and in most cases I use LLM, to come up with right evaluations.\n[05:45 -> 05:53] Here I can take, say, 01 or 03 now and just reverse engineer what should be the user\n[05:53 -> 05:59] question that we know to answer based on these materials and what should be the\n[05:59 -> 06:07] specific criteria that these materials provide an answer for. Some of these criteria are quite\n[06:07 -> 06:13] important. For example, here it says that as part of the thing, you need to receive a\n[06:13 -> 06:19] mobile validation, so you receive a SMS code, and it says that if you don't have\n[06:19 -> 06:25] a mobile number then you can reach support, et cetera, et cetera. If some of that\n[06:25 -> 06:31] information is missing from the answer, the answer would not be correct, but you need to\n[06:31 -> 06:37] be very specific about what exact information you need to see in the answer, and that information\n[06:37 -> 06:46] is very specific to that specific question. So you need to build lots of valves from the\n[06:46 -> 06:53] materials in this case that mimic specific user questions that you need to be able\n[06:53 -> 07:03] to answer for. How do we do it usually? Again, I work with smart models like 03, and I provide\n[07:03 -> 07:08] it enough context, I provide it which personas are we trying to represent, because you can\n[07:08 -> 07:14] make, ask the same question in completely different ways, depending on who is the\n[07:14 -> 07:19] persona asking, yet you would expect exactly the same answer, so you need to account\n[07:19 -> 07:29] for it. So this is an example from the open source platform that we have that just helps\n[07:29 -> 07:35] to get it done. So if you look it up, I'm not trying to sell you anything, I'm not trying\n[07:35 -> 07:41] to like vendor lock in or whatever, it's completely open source, and if needed I can\n[07:41 -> 07:45] just recreate it in a couple of days now with cursor, the point is in the approach,\n[07:45 -> 07:53] not in the platform. So for example here we see that very same question, how do I reset\n[07:53 -> 08:00] my password, you see what was the input, what was the output, and that specific criteria\n[08:01 -> 08:09] that I measure it, that specific question, how do I know if the answer is correct? And\n[08:09 -> 08:14] now I can just reiterate and generate like 50 different variations of the same question",
  "analysis": "## 1) **Approach Script (3 sentences)**\n\"Welcome to our discussion on practical tactics for building reliable AI applications. With my 15 years of experience as a startup co-founder and CTO, I've developed a unique approach to this challenge. Let's delve into why this method isn't widely adopted yet and how it can revolutionize your AI application development process.\"\n\n## 2) **Five High-Signal Questions**\n- At [00:30], you mentioned that nobody does it this way yet. Why do you think that is?\n- At [01:23], you mentioned the standard software development lifecycle. How does AI application development differ from this?\n- At [02:11], you mentioned that models are non-deterministic. How does this affect the reliability of AI applications?\n- At [03:16], you talked about measuring groundness, factuality, and bias. How do these metrics influence the effectiveness of AI applications?\n- At [04:30], you suggested reverse engineering metrics. Can you provide an example of how this has been beneficial in your experience?\n\n## 3) **Timeline Highlights (8â€“12 bullets)**\n- [00:03] Introduction to the topic of building reliable AI applications\n- [00:31] Speaker's background as a startup co-founder and CTO\n- [01:04] Mention of the importance of reliability in AI applications\n- [01:40] Discussion on the challenges of proof of concept (POC) with AI\n- [02:20] Explanation of the need for continuous experimentation in AI development\n- [03:00] Critique of the common approach of starting with data science metrics\n- [04:22] Suggestion to start with real-world scenarios and reverse engineer metrics\n- [05:06] Example of a customer support bot and the complexities of its evaluation\n- [06:46] Explanation of the need to build specific evaluation criteria for each user question\n- [07:29] Introduction to an open-source platform for AI development\n\n## 4) **Key Claims, Assumptions, Trade-offs**\n- **Claims (assertions)**\n  - AI applications can be made reliable through a specific approach.\n  - Traditional data science metrics may not provide a comprehensive understanding of an AI solution's effectiveness.\n  - Real-world scenarios and reverse-engineered metrics can lead to more effective AI applications.\n- **Assumptions (constraints implied)**\n  - AI development differs significantly from the standard software development lifecycle.\n  - Developing reliable AI applications requires continuous experimentation.\n  - The effectiveness of an AI solution should be measured by its ability to meet specific user needs.\n- **Trade-offs (gains vs sacrifices)**\n  - Gains: More reliable AI applications, better understanding of user needs, improved business outcomes.\n  - Sacrifices: Time and resources spent on continuous experimentation, reverse-engineering metrics, and creating specific evaluation criteria.",
  "word_count": 1194,
  "success": true
}