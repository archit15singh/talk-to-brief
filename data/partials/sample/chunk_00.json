{
  "chunk_index": 0,
  "chunk_text": "[00:14 -> 00:22] Welcome everyone. I'm going to talk about practical tactics to build reliable AI applications.\n[00:22 -> 00:30] And why nobody does it this way yet? A little bit about myself or why you should trust me.\n[00:31 -> 00:39] I've allowed 15 years as a startup co-founder and CEO. I held executive positions for the last five\n[00:39 -> 00:47] years at several enterprises. But most importantly, I spent last couple of years developing a lot of\n[00:47 -> 00:58] GNI projects, ranging from POCs to many production level solutions and helped some companies to get it done.\n[00:59 -> 01:09] And I've learned or distilled a way to make these applications reliable. And there are quite a lot of\n[01:09 -> 01:21] trucks, this conference about evils and reliability. But to my surprise nobody was talking about the most important things.\n[01:22 -> 01:33] And we're going to talk about it right now. So, standard software development lifecycle is a very standard simple.\n[01:33 -> 01:44] You design your solution, you develop it, you test it, and then eventually you deploy it. And when people start doing POC with AI,\n[01:45 -> 01:57] it sounds simple. You can very easily do some prompt and models are very capable. But then you start facing some unexpected challenges.\n[01:59 -> 02:13] Actually, you can easily do a POC that works 50% of the time. But making it do the same reliable work, the rest of the 50% is very hard, because models are non-deterministic.\n[02:14 -> 02:25] And it starts requiring a data science approach, continuous experimentation. You need to try this prompt, you need to try that model, you need to try this approach, etc, etc.\n[02:25 -> 02:39] And everything in your solution, everything that represents your solution, which is your code, your logic, the prompt, the models that you use, the data that you base your solution on.\n[02:40 -> 02:46] Changing anything of that impacts your solution in unexpected ways.\n[02:50 -> 03:01] People often come to this, to try solving this with the wrong approach. They start with data science metrics.\n[03:01 -> 03:21] It sounds reasonable, so it requires data science approach of the experimentation, and people start measuring groundness, factuality, bias, and other metrics that don't really help you to understand is your solution working the right way.\n[03:21 -> 03:29] Does your latest change improve your solution in the right way, for your users?\n[03:30 -> 03:39] For example, I've been talking to an ex colleague that are building a customer support boat at weeks. I asked him, how do you know that your solution is working well?\n[03:40 -> 03:44] He started talking about factuality and other data science metrics.\n[03:46 -> 04:02] That's again, I started to dig deeper, and then we just together figure out that the most important metric for them is the rate of moving from AI support boat, like escalation to human support.\n[04:03 -> 04:17] If your solution hasn't able to answer the user with all this factuality, it could be super grounded, but still not provide the right answer that the user expects, and this is what you actually need to test.\n[04:21 -> 04:41] My experience was to start with real-world scenarios. You need to reverse engineer your metrics, and your metrics should be very specific to what your end goal should come from a product experience from business outcomes.\n[04:41 -> 04:48] If your solution is customer support boat, you need to figure out what your users want and how you can mimic it.\n[04:49 -> 05:01] Instead of measuring something average or something generic, you need to measure a very specific criteria, because universal evolves don't really work.\n[05:01 -> 05:11] How do we do it? For example, customer support boat, which is by way one of the hardest things to do evolves properly.\n[05:12 -> 05:22] Let's say I have a bank, and a bank has FAQ materials, which contain, including how do you reset your password?\n[05:24 -> 05:37] What I usually do when I help my companies with help them to build AI solutions, we start with reverse engineering, like how do we create VALs based on that?\n[05:37 -> 05:44] In this case, I use LLM, and in most cases I use LLM, to come up with right evaluations.\n[05:45 -> 06:04] Here I can take, say, O3 now and just reverse engineer what should be the user question that we know to answer based on these materials, and what should be the specific criteria that these materials provide an answer for?\n[06:04 -> 06:23] Some of these criteria are quite important, so for example, here it says that as part of the thing, you need to receive a mobile validation, so you receive a SMS code, and it says that if you don't have a mobile number then you can reach support, et cetera, et cetera.\n[06:24 -> 06:40] If some of that information is missing from the answer, the answer would not be correct, but you need to be very specific about what exact information you need to see in the answer, and that information is very specific to that specific question.\n[06:40 -> 06:54] So you need to build lots of VALs from the materials, in this case, that mimic specific user questions that you need to label to answer for.\n[06:55 -> 07:19] How do we do it usually? Again, I work with smart models like O3, and I provide it enough context, I provide it which personas are we trying to represent, because you can make ask the same question in completely different ways, depending on who is the persona asking, yet you would expect exactly the same answer, so you need to account for it.\n[07:22 -> 07:46] So this is an example from the open source platform that we have that just helps to get it done, so if you look it up, I'm not trying to sell you anything, I'm not trying to vendor lock in or whatever, it's completely open source, and if needed, I can just recreate it in a couple of days now with cursor, the point is in the approach, not in the platform.\n[07:47 -> 08:08] So for example here, we see that very same question, how do I reset my password, you see what was the input, what was the output, and that specific criteria that I measure it, that specific question, how do I know if the answer is correct.\n[08:08 -> 08:22] And now I can just reiterate and generate like 50 different variations of the same question, and see if I still get the right answer, if the answer matches all the checklist that I have for that specific answer.\n[08:25 -> 08:36] How the process usually works, so contrary to regular approach, you build your evolves, not at the end of the process, but at the very beginning of the process.\n[08:36 -> 08:54] So you just build your first version of the POC, you define a first version of your tests, evaluations, you run them, and you see what's going on, you will see that in some cases it will fail, in some cases it will succeed.\n[08:54 -> 09:05] What's important is to look at the details, not just see the average numbers, the average numbers won't tell you anything, won't tell you how to improve it.\n[09:05 -> 09:15] If you actually look at the details of each evaluation, you'll see exactly why it's failing, it could be failing because your test is not defined correctly.",
  "analysis": "## 1) **Approach Script (3 sentences)**\n\"Welcome to our discussion on building reliable AI applications, a topic that is often overlooked in the industry ([00:14 -> 00:22]). As a seasoned startup co-founder and executive, I've spent years developing GNI projects and have distilled a method to make these applications reliable ([00:31 -> 00:58]). Let's delve into the challenges and solutions in the AI development lifecycle, and why the right metrics are crucial for success ([01:22 -> 01:57]).\"\n\n## 2) **Five High-Signal Questions**\n- What are the common challenges faced when transitioning from POC to production level in AI applications? ([01:45 -> 01:57])\n- How does changing any aspect of the solution (code, logic, models, data) impact the solution in unexpected ways? ([02:25 -> 02:39])\n- Why do traditional data science metrics fail to provide a clear understanding of the solution's effectiveness? ([03:01 -> 03:21])\n- How can real-world scenarios and specific user needs guide the development of more effective metrics? ([04:21 -> 04:48])\n- How does providing context and understanding user personas improve the effectiveness of AI solutions? ([06:55 -> 07:19])\n\n## 3) **Timeline Highlights (8â€“12 bullets)**\n- [00:14 -> 00:22] Introduction to the topic: building reliable AI applications.\n- [00:31 -> 00:39] Speaker's background as a startup co-founder and executive.\n- [01:09 -> 01:21] Observation on the lack of focus on reliability in AI discussions.\n- [01:45 -> 01:57] Challenges faced when moving from POC to production in AI.\n- [02:14 -> 02:25] Need for a data science approach and continuous experimentation.\n- [03:01 -> 03:21] Critique of traditional data science metrics.\n- [04:03 -> 04:17] Importance of real-world scenarios and user-focused metrics.\n- [05:37 -> 05:44] Use of LLM for evaluations.\n- [06:55 -> 07:19] Importance of context and user personas in AI solutions.\n- [08:08 -> 08:22] Iterative process of testing and refining AI solutions.\n- [09:05 -> 09:15] Importance of detailed evaluation over average numbers.\n\n## 4) **Key Claims, Assumptions, Trade-offs**\n- **Claims (assertions)**\n  - AI applications can be made reliable with the right approach.\n  - Traditional data science metrics often fail to provide a clear understanding of an AI solution's effectiveness.\n  - Real-world scenarios and user-focused metrics are crucial in AI development.\n- **Assumptions (constraints implied)**\n  - The speaker assumes that the audience understands the basic concepts of AI and data science.\n  - The speaker assumes that the audience is interested in or has a need for reliable AI applications.\n  - The speaker assumes that the audience agrees with the need for a new approach to AI development.\n- **Trade-offs (gains vs sacrifices)**\n  - By focusing on real-world scenarios and user-focused metrics, developers may need to spend more time on the initial stages of development, but this could lead to more reliable and effective AI applications.\n  - By moving away from traditional data science metrics, developers may lose some standardization but gain a more nuanced understanding of their AI solution's effectiveness.",
  "word_count": 1282,
  "success": true
}