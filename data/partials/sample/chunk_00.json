{
  "chunk_index": 0,
  "chunk_text": "[00:03 -> 00:22] Welcome everyone. I'm going to talk about practical tactics to build reliable AI applications\n[00:23 -> 00:30] and why nobody does it this way yet. A little bit about myself or why you should trust me.\n[00:31 -> 00:42] I'm about 15 years as a startup co-founder and CTO. I held executive positions for the\n[00:42 -> 00:54] last couple of years developing a lot of projects ranging from POCs to many production-level solutions\n[00:54 -> 01:04] and helped some companies to get it done. I've learned or distilled a way to make these\n[01:04 -> 01:15] applications reliable. There are quite a lot of tracks this conference about evals and reliability\n[01:15 -> 01:23] but to my surprise nobody was talking about the most important things and we're going to talk\n[01:23 -> 01:34] about it right now. Standard software development lifecycle is very standard, simple. You design\n[01:34 -> 01:40] your solution, you develop it, you test it and then eventually you deploy it. When people start\n[01:40 -> 01:51] doing POC with AI, it sounds simple. You can very easily do some prompt and models are very\n[01:51 -> 02:01] capable but then you start facing some unexpected challenges. Actually, you can easily do a POC\n[02:01 -> 02:11] that works 50% of the time but making it do the same reliable work the rest of the 50% is very hard\n[02:11 -> 02:20] because models are non-deterministic and it starts requiring a data science approach, continuous\n[02:20 -> 02:24] experimentation. You need to try this prompt, you need to try that model, you need to try this\n[02:24 -> 02:32] approach, etc. Everything in your solution, everything that represents your solution which is your\n[02:32 -> 02:38] code, your logic, the prompts that you use, the models that you use, the data that you base your\n[02:38 -> 02:51] solution on, changing anything of that impacts your solution in unexpected ways. People very\n[02:51 -> 03:00] often come to this, to try solving this with the wrong approach. They start with data science\n[03:00 -> 03:06] metrics. It sounds reasonable, right? So it requires data science approach of the experimentation\n[03:07 -> 03:16] and people start measuring groundness, factuality, bias and other metrics that don't really help\n[03:16 -> 03:24] you to understand is your solution working the right way? Does it, does your latest change\n[03:26 -> 03:31] improved your solution in the right way for your users? For example, I've been talking\n[03:31 -> 03:37] to an ex-colleague that are building a customer support bot at Weeks. I asked him, how do\n[03:37 -> 03:42] you know that your solution is working well? He started talking about factuality and\n[03:42 -> 03:50] other data science metrics. That's again, I started to dig deeper and then we just together\n[03:51 -> 03:59] figure out that the most important metric for them is the rate of moving from AI support\n[03:59 -> 04:07] bot like escalation to a human support. If your solution hasn't able to answer the\n[04:07 -> 04:12] user with all this factuality, like it could be super grounded but still not provide\n[04:12 -> 04:22] the right answer that user expects and this is what you actually need to test. My experience\n[04:22 -> 04:30] was to start with real-world scenarios. Basically, you need to reverse engineer your metrics\n[04:30 -> 04:37] and your metrics should be very, very specific to what your end goal, so they should come\n[04:37 -> 04:44] from a product experience, from business outcomes. If your solution is customer support bot, you\n[04:44 -> 04:51] need to figure out what your users want and how you can mimic it. Instead of measuring\n[04:51 -> 04:57] something average or something generic, you need to measure a very specific criteria\n[04:58 -> 05:06] because universal valves don't really work. How do we do it? For example, customer\n[05:06 -> 05:13] support bot, which is by way one of the hardest things to do the valves properly. Let's say\n[05:13 -> 05:21] I have a bank and a bank has FAQ materials which contain, including how do you reset your\n[05:21 -> 05:31] password? What I usually do when I help my companies that I help them to build AI\n[05:31 -> 05:37] solutions, we start with reverse engineering, like how do we create the valves based on that?\n[05:38 -> 05:44] In this case, I use LLM, and in most cases I use LLM, to come up with right evaluations.\n[05:45 -> 05:53] Here I can take, say, 01 or 03 now and just reverse engineer what should be the user\n[05:53 -> 05:59] question that we know to answer based on these materials and what should be the\n[05:59 -> 06:07] specific criteria that these materials provide an answer for. Some of these criteria are quite\n[06:07 -> 06:13] important. For example, here it says that as part of the thing, you need to receive a\n[06:13 -> 06:19] mobile validation, so you receive a SMS code, and it says that if you don't have\n[06:19 -> 06:25] a mobile number then you can reach support, et cetera, et cetera. If some of that\n[06:25 -> 06:31] information is missing from the answer, the answer would not be correct, but you need to\n[06:31 -> 06:37] be very specific about what exact information you need to see in the answer, and that information\n[06:37 -> 06:46] is very specific to that specific question. So you need to build lots of valves from the\n[06:46 -> 06:53] materials in this case that mimic specific user questions that you need to be able\n[06:53 -> 07:03] to answer for. How do we do it usually? Again, I work with smart models like 03, and I provide\n[07:03 -> 07:08] it enough context, I provide it which personas are we trying to represent, because you can\n[07:08 -> 07:14] make, ask the same question in completely different ways, depending on who is the\n[07:14 -> 07:19] persona asking, yet you would expect exactly the same answer, so you need to account\n[07:19 -> 07:29] for it. So this is an example from the open source platform that we have that just helps\n[07:29 -> 07:35] to get it done. So if you look it up, I'm not trying to sell you anything, I'm not trying\n[07:35 -> 07:41] to like vendor lock in or whatever, it's completely open source, and if needed I can\n[07:41 -> 07:45] just recreate it in a couple of days now with cursor, the point is in the approach,\n[07:45 -> 07:53] not in the platform. So for example here we see that very same question, how do I reset\n[07:53 -> 08:00] my password, you see what was the input, what was the output, and that specific criteria\n[08:01 -> 08:09] that I measure it, that specific question, how do I know if the answer is correct? And\n[08:09 -> 08:14] now I can just reiterate and generate like 50 different variations of the same question",
  "analysis": "## 1) **Approach Script**\nWelcome to this talk on practical tactics for building reliable AI applications. The speaker, an experienced startup co-founder and CTO, will discuss why many current approaches fall short and how to better align AI development with real-world scenarios and specific user needs. Let's delve into the nuances of this process and uncover the key elements of a successful AI solution.\n\n## 2) **Five High-Signal Questions**\n1. What are the common challenges faced when transitioning from a POC to a production-level AI solution? (Timestamp: 01:51 -> 02:01)\n2. Why do traditional data science metrics often fail to accurately measure the effectiveness of an AI solution? (Timestamp: 03:07 -> 03:16)\n3. How can real-world scenarios and specific user needs guide the development of more reliable AI applications? (Timestamp: 04:22 -> 04:30)\n4. What is the importance of reverse engineering in the context of AI solution development? (Timestamp: 05:31 -> 05:37)\n5. How can open-source platforms aid in the development of reliable AI applications? (Timestamp: 07:29 -> 07:35)\n\n## 3) **Timeline Highlights**\n- [00:03 -> 00:22] Speaker introduces the topic: practical tactics for building reliable AI applications.\n- [00:31 -> 00:42] Speaker shares his background as a startup co-founder and CTO.\n- [01:04 -> 01:15] Speaker notes the lack of discussion on key aspects of AI reliability.\n- [01:51 -> 02:01] Challenges of transitioning from POC to production-level AI solutions are discussed.\n- [03:00 -> 03:06] Speaker criticizes the over-reliance on traditional data science metrics.\n- [04:22 -> 04:30] Emphasis on the importance of real-world scenarios and specific user needs in AI development.\n- [05:31 -> 05:37] Introduction of the concept of reverse engineering in AI solution development.\n- [06:53 -> 07:03] Speaker discusses working with smart models like 03.\n- [07:29 -> 07:35] Speaker introduces an open-source platform that aids in AI development.\n- [07:53 -> 08:00] Demonstration of how specific criteria are measured in AI solution development.\n\n## 4) **Key Claims, Assumptions, Trade-offs**\n\n**Claims:**\n- Transitioning from a POC to a production-level AI solution presents unexpected challenges.\n- Traditional data science metrics often fail to accurately measure the effectiveness of an AI solution.\n- Real-world scenarios and specific user needs should guide the development of AI applications.\n\n**Assumptions:**\n- AI solutions should be reliable and capable of handling real-world scenarios.\n- The effectiveness of an AI solution can be measured by its ability to meet specific user needs.\n- Open-source platforms can aid in the development of reliable AI applications.\n\n**Trade-offs:**\n- Focusing on real-world scenarios and specific user needs may require more time and resources but leads to more reliable AI applications.\n- Using traditional data science metrics may seem reasonable but often fails to accurately measure the effectiveness of an AI solution.\n- Utilizing open-source platforms can save development time but may require a certain level of technical expertise.",
  "word_count": 1194,
  "success": true
}